## 复习知识

* 异常值检测的算法
  * 定义--与均值相差两个标准差
  * 箱线图的方式
  * grubb's test
  * dixon's 
  * t检验
* 机器学习中距离的种类
* 异常检查的方式
* 聚类方法
  * 将抽象的集合分组为由类似的对象组成的多个类的分析过程
  * 评价
    * 计算优势，处理的数据集形状是否有限制
    * 数据噪声的处理能力
    * 预先的类的个数是否知道，用户是否需要用户给出领域知识
  * 聚类方法
    * 划分方法
      * 基于距离的分组方法，划分为n个簇
      * k均值和k中心法
    * 层次聚类
      * 类的个数是不断变化的，从小的堆变成大的堆
      * 类之间的距离衡量
    * 基于密度的方法
      * DBSCAN
    * 基于模型的方法
      * GMM
* 机器学习中常用的距离定义
  * 闵可夫斯基距离
  * 欧氏距离
  * Manhattan距离
  * 切比雪夫距离
  * 余弦相似度
  * 汉明距离
  * KL散度
  * Jaccard矩阵
  * 马氏距离中的推导过程
    * 考虑了线性变换后的方差和期望
    * 期望的表示
    * 和的向量化
    * 消除量纲和相关性的影响，更多地考虑到分布
    * 可以理解为PCA，因为Q就是相互正交的矩阵，但是选择可以不同，但最好的方式还是PCA
    * 核心就是消除量纲和相关性，相关性通过坐标变换，量纲即归一化，然后欧式距离即可
* 信息熵
  * 熵即衡量事物的随机程度的量，不确定程度的度量
  * 熵最大即等概率的原则
* 决策树算法
  * 决策数算法是一个贪心的算法，寻求每一步的最优化来达到整体的最优
    * 判断的标准为纯度和不纯度
    * 纯度的计算方式为基尼系数或者熵，熵的惩罚程度更重，偏向过拟合
    * 随机森林在这一步会加上随机性
  * 决策树比较重要的步骤
    * 分叉的问题，分几个叉，但通常是决策树桩
    * 分支的判断标准
      * 通过这两个步骤可以调控很大参数来控制过拟合的程度
      * 例如margin的效果，最小的分支个数
    * 叶子节点的值，即递归的返回结果
      * 类别的多类问题和均值的问题
    * 过拟合的剪枝问题
  * 实例的处理过程
    * ID3
      * 信息增益准则选择特征
      * 处理离散型的变量较为合适，连续型数据需要进行离散化的处理
      * 缺省值没有考虑
      * 没有考虑剪枝的问题
    * C4.5
      * 信息增益率的计算
      * 自动离散化的处理过程
      * 对于缺失样本的处理方式
        * 系数的调整
        * 另一种考虑
      * 正则化系数的剪枝过程
    * CART
      * 决策树桩
      * 基尼系数作为建树的标准
      * 回归树的话选用比较常用的均方误差
      * 停止分裂的条件
        * 最小节点数
        * 熵或者基尼小于阈值
        * 决策树的深度够了
        * 特征用完了 
      * 缺省值的处理过程
        * 样本某些特征值缺失的情况下选择划分的属性
        * 选定了划分属性，然后样本的分配问题
      * 重要性的计算方法
        * 用gini系数来代表单位的不纯度，然后计算总体的降低，利用降低的能力均一化
* 过拟合
  * 过拟合指模型的复杂度和能力太过强大，对训练数据的拟合程度太高
  * 增大数据量
  * 取决于基本的模型，正常的数值模型，非结构型
  * 正则化的处理
    * 通常是L1和L2处理，使w趋于0
  * 神经网络
    * L1和L2
    * batch norm
    * dropout
  * 树模型的话
    * 深度，熵等等
  * 对于不建模的KNN
    * 可以选择增加N的个数，减少过拟合
* 逻辑回归
  * 逻辑回归最简单的认识方式就是一个激活函数，sigmoid强制把数值压缩为概率，但这种认识是浅薄的
  * 从贝叶斯的角度来看，逻辑回归成立的条件是假设了方差的相等，然后对对数似然进行了建模
    * 从结果的形式发现它可以是一个线性函数
  * 逻辑回归对概率值的输出效果很好，在概率校准曲线中的表现很好
  * 交叉熵损失的推导
    * 利用组合的形式和最大似然估计的思想
    * 在sigmoid下也可以具体的推导
    * $g(-s) = 1 - g(s)$
* 机器学习模型
  * 监督学习
    * 连续型标签 --- 回归问题
      * 对于回归问题，通常的错误的检验方法就是均方误差的形式
      * 处于统计学的考虑，也可以有R2的解释，这是两种比较常见的衡量方式
      * 对于一些绝对值的和，等等都是类似的含义
    * 离散标签 --- 分类问题
      * 对于分类问题来讲，二分类是基础，然后推广到多分类
      * 二分类的方法，直接输出概率，直接输出标签的形式
        * 直接输出标签
          * 感知机算法
          * 决策树模型
          * 以线性模型的值大于0否的分类都为直接输出标签的形式
          * 这种模型衡量通常是精确率，或者混淆矩阵来观察分析
          * 利用ROC曲线和AUC面积
        * 输出概率和调整阈值
          * 逻辑回归
          * 调整的SVM也可以达到类似的效果
          * 纯概率的贝叶斯方法
          * 总体来说，逻辑回归的结果要好一点
          * 评判方法
            * 对于概率来说，即随机变量，可以利用交叉熵的方法
            * 同样，还要布里尔分数的概念，就是均方误差的概念
              * 这里一定要注意类别和有序变量的区别
              * 通常用于二分类
            * 用概率校准曲线也可以来看其中的区别
  * 无监督学习
    * 聚类问题的评估方法
      * 标签已知的情况下
        * 互信息
        * V-measure
        * 调整兰德系数
      * 无标签
        * 轮廓系数
  * 半监督学习
* 评分卡
  * WOE
    * 证据权重，从贝叶斯的角度来看

